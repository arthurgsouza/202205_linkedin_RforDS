---
title: "Dataprep"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Considerações Finais 
Além das soluções e bibliotecas aqui comentados, é importante lembrar que o R oferece muitas formas de completar uma mesma atividade. Sendo assim, é sempre recomendado que procure opções para otimizar sua rotina. Seguem abaixo as algumas referências open-source para o aprofundamento dos pontos aqui listados:

- R for data science: https://r4ds.had.co.nz/introduction.html
- Advanced R: http://adv-r.had.co.nz/
- Hands-On Programming with R: https://rstudio-education.github.io/hopr/
- Curso-R: https://www.curso-r.com/material/
 
 
# Programação
Como ferramenta transversal a todas as etapas do workflow de análise, temos a programação funcional, que permite a criação de códigos mais rápidos, simples e reproduzíveis. No contexto do `tidyverse` a biblioteca `purrr` disponibiliza de uma série de funcionalidades visando aprimorar a programação funcional no R. Como principal função temos o `map()`, transformando a sua entrada por meio da aplicação de uma função a cada elemento, e retornando uma lista com o mesmo comprimento que a entrada:


```{r}
#exemplo: , contabilização de tal identificação de dados faltantes por coluna, 
  starwars %>% 
    map(is.na) %>% #identificação de quais elementos, de cada coluna, são NA's
    map(sum) %>% #contabilização dos dados faltantes por coluna  
    glimpse() #sumarização do resultado

#outras opções de sintaxe para obter o mesmo resultado
  #starwars %>% map(~sum(is.na(.)))
  #starwars %>% map( function(x) sum(is.na(x)) )
```

O `map()` retorna listas por default, para que o resultado tenha outra estrutura/classe, podemos utilizar funções como: `map_dbl` que retorna um vetor número, `map_dfc` para um data.frame que combina os resultados por coluna, entre outras opções. Adicionalmente temos recursos como:

  - `walk()` - similar ao `map`, porém sem ter o retorno da lista no console 
  - `pmap()` - aplica uma função a um grupo de elementos de um grupo de listas
  - `append()` - adiciona valores ao fim de uma lista


```{r, eval=F, warning = F}
#exemplo: para visulizar todos os arquivos de um dado diretório
  list.files("\diretório", pattern='*.xlsx') %>% 
    map(read_excel) %>% 
    walk(glimpse)
```



# R para Big Data
O R, por padrão, trabalha com dados carregados em memória, por meio de processamento sequencial, o que pode limita tanto o tamanho da base de dados que podemos trabalhar, quanto a velocidade de análise. Alternativas para quando estamos sob alguma destas limitações incluem: trabalhar com amostragens, usar um computador com mais recursos, acessar os dados considerando outras formas de leitura/processamento, ou segmentação de dados. Detalhando estas duas últimas opções temos:

- outras formas de acesso: 
  - substituir os data.frames/tibbles pelo data.table, da biblioteca `data.table()`, uma vez que este trabalha com otimização de processamento e memória;
  - processar os dados em disco, ao invés da memória, por meio de bibliotecas como `bigmemory`, `ff` ou `RevoScaleR`;
  - dado que o R é por padrão uma linguagem interpretada, utilizar funções como `compile()` ou `enableJIT()` para acelerar o processamento de uma análise, principalmente se ela for executada repeditamente; e
  - utilizar pacotes que possibilitam paralelismo, como `Foreach` ou `Multicore`.

- segmentação: aqui podemos trabalhar "manualmente", carregando sub-conjuntos das base de dados, analisando cada uma delas, e depois combinando os resultados. Ou carregar bibliotecas que permitam integração do R com o ecossistema de Big Data, permitindo a distribuição e análise de forma automática. O `RHadoop`, por exemplo, é um conjunto de pacotes que permite conectar o ambiente R em clusters HDFS e HBASE para a execução de operações. Outra opção é o `sparklyr`, que possibilita a execução de comandos do R em clusters do Apache Spark.
Tanto o `RHadoop` quanto o `sparklyr` possibilitam o uso de comandos do pacote `dplyr`, e a sintaxe do `tidyverse()`. Assim, as tecnologias se complementam: é possível escrever código de forma mais simples e rápida no R e executá-lo distribuidamente em grandes bases de dados no Hadoop ou Spark, e podemos alavancar os tradicionais sistemas de Big Data, integrando a eles as funcionalidades de análise e machine learning disponíveis no ecossistema do R.

Visando ilustrar a facilidade de tais integrações, segue a aplicação do mesmo conjunto de códigos anteriormente utilizados para a base `mtcars`:


```{r}
# #carregando da biblioteca após sua instalação
# library(sparklyr)
# library(dplyr)
# 
# #criando uma conexão
# spark_conexao <- spark_connect(master = "local")
# 
#  #transformando os dados de memŕia numa tbl_spark
#   mtcars_tbl <- sdf_copy_to(spark_conexao,
#                             mtcars, name = "mtcars_tbl",
#                             memory = FALSE)
#   class(mtcars_tbl)
# 
#  #verificando a estrutura dos dados
#   mtcars_tbl %>% glimpse()
# 
#  #selecionando alguma variaveis usando o dplyr
#   mtcars_reduzido <- mtcars_tbl %>%
#     select(mpg, hp)
# 
#  #salvando o mtcars_reduzido em memória
#   mtcars_reduzido_ram <- collect(mtcars_reduzido)
# 
# #Disconnect from Spark
# spark_disconnect(sc = spark_conexao)
```

Vale ressaltar que muitas das soluções que envolvem Big Data no R são novas, assim como o ecosistema de Big Data em si. E, portanto, estão em constante desenvolvimento, aumentando ainda mais a importância de manter o tracking de melhorias e correções dos pacotes. Vale ressaltar que mesmo com toda a necessidade de ajustes aqui apresentada, a quantidade de recursos que o R oferece para o flow da Ciência de Dados, faz com que grandes empresas, como Google, Microsoft, Bank of America e Shell, optem por trabalhar com a linguagem R.



# Boa sorte! 
