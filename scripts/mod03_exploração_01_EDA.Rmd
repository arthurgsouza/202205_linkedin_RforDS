---
title: "EDA - Exploratory Data Analysis (Análise Exploratória de Dados)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
```

# Bibliotecas

```{r}
library(tidyverse)
```

# EDA

EDA, do inglês Exploratory Data Analysis, e em português Análise Exploratória de Dados, é o processo de explorar os dados, o que tipicamente inclui desde examinar a estrutura do dataset, algo que iniciamos no módulo anterior, até o entendimento mais profundo sobre o comportamento das variáveis. A EDA nos permite:

- Observar padrões não necessariamente conhecidos a priori
- Identificar se há algum problema com o seu conjunto de dados
- Determinar se a pergunta que você está fazendo pode ser respondida pelos dados que você possui
- Desenvolver uma esboço da resposta à sua pergunta, por vezes respondê-la 

Aqui, é possível, tanto ir em uma linha de exploração mais descritiva, por meio de volumetrias, estatísticas e visualizações, quanto ir em uma linha mais diagnóstica, investigando comportamentos mais complexos, como padrões de associação, por exemplo. Em linhas gerais, é nesta etapa que *aprendemos* e damos *sentido* aos dados! 



# Dados

Vamos trabalhar com uma base de comerciais do Superbow, considerando os anúncios das 10 marcas que mais veicularam nos Super Bowls dos últimos anos, tendo, para cada comercial, uma classificação binária relacionada a sete critérios pré-especificados. Para mais informações sobre o levantamento e a base de dados acesse:

- <https://projects.fivethirtyeight.com/super-bowl-ads/>

- <https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-03-02/readme.md>


```{r}
youtube <- readr::read_csv(
  'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')

```


```{r}
youtube %>% glimpse()
```





# Bibliotecas para EDA

Vamos iniciar fazendo uso de pacotes que nos ajudam a entender as principais características dos dados:


## janitor
Este pacote disponibiliza algumas funções para limpar e avaliar bases de dados, com funções como:

- `janitor::remove_empty_cols()` para a remoção de colunas vazias;
- `janitor::get_dupes()` para a identificação de linhas duplicadas; e
- `janitor::round_half_up()` para arredondar resultados numéricos.

Ah, tem uma função MUITO legal, `janitor::clean_names()` que limpa o nome das colunas. Para a base do SuperBow ela não é muito relevante, visto que os nomes já estão em um ótimo formato. Mas mais para frente passaremos por um exemplo em que esta função irá ajudar bastante. Apenas para ilustrar o seu uso, segue uma aplicação para a modição do estilo das colunas : 
  
```{r}
#janitor::
youtube %>% janitor::clean_names(case = "upper_camel") %>%  glimpse()
```

Este pacote nos possibilita criar tabelas de modo rápido:

```{r}
youtube %>% janitor::tabyl(year) 
```

E mais, deixar a tabela bonitinha :)

```{r}
youtube %>% 
  janitor::tabyl(brand) %>%  
  janitor::adorn_totals() %>%  
  janitor::adorn_pct_formatting()
```

Essa mesma função funciona também para visualizar duas variáveis por vez:

```{r}
youtube %>% 
  janitor::tabyl(year, brand) %>%  
  janitor::adorn_totals(c("row", "col")) 
```

## inspectdf

Este pacote traz uma série de recursos para apresentação de resumos dos dados, como volumetrias e dados faltantes, respeitando sempre a classe das variáveis – lembrando que já discutimos sobre o quão importante este conceito é, tanto conceitualmente, na estatística ou matemática, quanto no contexto computacional:

```{r}
#inspectdf::
youtube %>% inspectdf::inspect_types()
```
Apesar de não priorizarmos dados do tipo lista neste curso, vou deixar aqui um exemplo de como poderiamos consultar colunas do tipo lista:
```{r}
youtube %>% inspectdf::inspect_types() %>% pull(col_name)
```

Podemos também ter visibilidade das categorias (níveis) mais frequente de cada uma das variáveis categóricas:
```{r}
youtube %>% inspectdf::inspect_imb()
```

Ou ainda a versão gráfica da tabela acima:
```{r}
youtube %>% inspectdf::inspect_imb() %>% inspectdf::show_plot()
```

Bem como recursos sobre os dados faltantes por coluna:
```{r}
youtube %>% inspectdf::inspect_na()
```

Ou ainda medidas de resumo relacionadas às variáveis numéricas:

```{r}
youtube %>% inspectdf::inspect_num()
```


## skimr

Em português skim significa algo como "tirar a nata", no caso, tirar a nata dos dados. E é exatamente isto o que este pacote faz, visto que nos permitir uma visão macro de toda a base, com uma serie de estatísticas organizadas de acordo com a classe das colunas – inclusive com um gráfico de frequência! Muito legal né? :p 

```{r}
#skimr::
youtube %>%  skimr::skim()
```

# Como avaliar os resultados? 

Estatística! Aqui não tem muito para onde correr, é da estatística que vêm os ferramentais que utilizamos para sumarizar os dados. E como se trata de um tema amplo, e o nosso objetivo neste curso não é aprofundar no tema, vou deixar aqui as aulas remotas de Estatística Básica, ofertada pelo Departamento de Estatística da UFPR entre 2020 e 2021, para o caso de você ter interesse em aprofundar no tema: 

- [Aulas Remotas da Disciplina de Estatística Básica ofertada pelo DEST-UFPR ](https://www.youtube.com/playlist?list=PLQcLb-PUD9WNZnVBYDKEonioyJw3nEaOM)

Mas não se preocupe, iremos passar por alguns dos conceitos chave, dividindo a discussão em três momentos:

## 1. Medidas de Resumo (univariadas)

Aqui temos a essência do que se trata a estatística descritiva: resumir dados. De modo que independente de quantas observações você tenha na sua tabela, seja possível sumariza-las e ter conclusões a respeito das características dos dados. Mas note que assim como qualquer resumo, implica em uma simplificação, implica em perda de informação, e no caso da estatística descritiva não é difirente. Contudo, aqui, podemos nos cercar de medidas, de modo que, em conjunto, tais metricas nos permitam traçar o perfil dos dados. Dentre as principais medidas de resumo, temos: 

- Medidas de Posição, Concentração ou Tendência Central: média (μ), moda (Mo), mediana (Md)
- Medidas de Ordenação (posição relativa): máximo, mínimo, quartis (1º quartíl = q1, e 3º  = q3), decis e percentis.
- Medidas de Dispersão (absoluta): variância (σ²), desvio padrão (σ), amplitude interquartil (IQR)
- Medidas de Dispersão (relativa): coeficiente de variação (CV)
- Medidas de Forma: curtose e assimetria
- Medidas de dependência: covariância e correlação (ρ)


E para discutir melhor estes grupos, vamos considerar as colunas numéricas da base de comerciais do Superbowl:
```{r}
youtube %>% select(where(is.numeric)) 
```


Para facilitar a discussão vamos filtrar algumas linhas e salvar em um objeto à parte: 

```{r}
(youtube_dislike_count = 
  youtube %>% slice(20:30) %>% pull(dislike_count))
```

Observe a tabela de frequência destes dados, qual o valor mais frequente? 

```{r}
table(youtube_dislike_count)
```


Um parênteses aqui pra refletir: como poderiamos fazer uma tabela desta, no caso em que temos dados contínuos, ao invés de dados discretos?

Voltando aos nossos dados, vamos agora ordena-los:

```{r}
youtube_dislike_count %>% sort()
```

E fazer o cálculo de algumas estatísticas mais:


```{r}
youtube_dislike_count %>% skimr::skim() %>%  skimr::yank("numeric")
```

E agora, ao comparar a lista dos números ordenador com as estatísticas geradas, a média `r round(mean(youtube_dislike_count),2)` parece estar descrevendo bem os dados? Você diria que o valor 68 é uma representação da quantidade de 'dislikes' das propagandas? Um outro ponto que chama a atençao é o quão alto é o desvio padrão, no caso, `r round(sd(youtube_dislike_count),2)`. E o que aconteceu para que estes números fossem tão **misleading**, ou seja, tão enganosos em relação ao que ocorre na base de dados, e a resposta é: outlier. 
Este termo é utilizado no contexto em que um ou mais pontos estão demasiadamente distante dos demais, o que pode acabar por influenciar algumas medidas de resumo, como é o caso da média, ou do desvio padrão (que tem o seu calculo baseado na média) – uma forma de perceber a questão do outlier é notando o quão distante o máximo é das demais métricas de posição e/ou ordenação. Neste cenário, trabalhar com medidas de ordenação, como a mediana (`r median(youtube_dislike_count)`) para a caracterização da tendencia central, e a amplitude do intervalo interquartil (`r IQR(youtube_dislike_count)`) como medida de dispersão, são alternativas interessantes, visto serem métricas robustas a outliers. 


Por fim vamos comentar um pouco sobre a questão do absoluto vs. relativo, esta diferenciação é importante pois, ao trabalhar com métricas de natureza absoluta, vamos lidar com os resultados segundo a grandeza do dado original. Contudo, isto nem sempre é interessante, particularmente quando queremos comparar variáveis que possuam magnitudes muito diferentes. Por exemplo:

```{r}
youtube %>% select(dislike_count, view_count) %>% skimr::skim() %>%  skimr::yank("numeric")
```

Neste caso, o coeficiente de variação é mais informativo. Pois para as mesmas variáveis que vemos uma diferença significativa em termos de dispersão absoluta, ao trabalhar em termos relativos, a conclusão muda, com ambos os atributos apresentando dispersões similares:

```{r}
# dislike_count
(sd(youtube$dislike_count, na.rm = TRUE) / mean(youtube$dislike_count, na.rm = TRUE))

# view_count
(sd(youtube$view_count, na.rm = TRUE) / mean(youtube$view_count, na.rm = TRUE))
```


Um último grupo de medidas de resumo seriam as medidas de forma, caso do coeficiente de curtose ou de assimetria, porém, alternativamente, vamos focar na distribuição de frequência de uma variável, que irá nos dar uma visão mais madura sobre o que estas quantidades nos dizem sobre os dados. 


## 2. Distribuição Empírica dos Dados

Cada uma das estatísticas que vimos até aqui resume os dados em um único número. Isso tem uma vantagem prática, mas conforme também pudemos observar, tratam-se de visões parciais. Neste sentido, é útil explorar como os dados são distribuidos de maneira geral. Para isto, temos a distribuição empírica dos dados, que nos possibilita ter noção da frequência das observações. Antes de continuarmos, é importante pontuar que trata-se de um tema extenso, a partir do qual tópicos como: definição do intervalo de tabelas de frequência, densidade Kernel, distribuições de probabilidade, probabilidade a posteriori, e tantos outros poderiam ser abordados. Mas aqui vamos abstrair dos (importantes) detalhes técnicos, para focar na interpretação das distribuições empíricas. Nosso objetivo aqui é que você perceba a importância de avaliar a distribuição dos dados, considerando para isto características de: 

- Concentração: ponto(s) ou intervalo(s) de concentração das observações
- Dispersão: o quão próximo ou distante as observações estão entre si
- Outliers: observações que se diferenciam de maneira significativa das demais
- Forma: aspectos de forma, como simetria ou assimetria (à esquerda ou direita)


E para ilustrar esta discussão, considere a base de dados `lincoln_weather` do pacote `ggridges`, com informações meteorológicas de Lincoln, (EUA - Nebraska), no ano de 2016: 

```{r}
ggridges::lincoln_weather %>% glimpse()
```

Uma observação rápida: perceba o ganho de produtividade que a função `janitor::clean_names()` nos possibilita aqui.

```{r}
ggridges::lincoln_weather %>% 
  janitor::clean_names() %>% 
  mutate(media_temperatura_c = (mean_temperature_f - 32)/1,8) %>% 
  ggplot(aes(x = media_temperatura_c, y = month, fill = stat(x))) +
  ggridges::geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  scale_fill_viridis_c(name = "Temp. [C]", option = "C") +
  coord_cartesian(clip = "off") +
  labs(title = 'Temperaturas em Lincoln (USA-NE) no ano de 2016') +
  ggridges::theme_ridges(font_size = 13, grid = TRUE) +
  theme(axis.title.y = element_blank())
```

Agora comparando a distribuição dos dados, com as suas respectivas medidas de resumo.
```{r}
ggridges::lincoln_weather %>% 
  janitor::clean_names() %>% 
  mutate(media_temperatura_c = (mean_temperature_f - 32)/1,8) %>% 
  select(month, media_temperatura_c) %>% 
  group_by(month) %>% 
  skimr::skim() %>%  
  skimr::yank("numeric")
```


Como mensagem final, vou citar Laplace: 

> "The theory of probabilities is at bottom nothing but common sense reduced to calculus; it enables us to appreciate with exactness that which accurate minds feel with a sort of instinct for which ofttimes they are unable to account" (Laplace, 1812)

Algo como: "No fundo, a teoria das probabilidades é apenas o sesnso comum expresso em números; nos permite apreciar com exatidão o que as mentes acuradas sentem com uma espécie de instinto pelo qual muitas vezes são incapazes de explicar". Pois bem, agora você é capaz :)



## 3. Medidas de Associação 

Scatter plot

Por fim vamos comententar de medidas que nos permitem reduzir a um único número a relação entre duas variáveis:

- Medidas de Dependência ou Associação (absoluta): covariância (cov)
- Medidas de Dependência ou Associação (relativa): correlação (r ou ρ)

No caso da covariância temos a questão de ser uma métrica absoluta, tornando díficil interpretar a sua magnitude. Enquanto que a correlação pode ser entendido como uma versão normalizada da covariância, permitindo uma interpretação muito  mais direta, visto variar -1 e 1. No caso em que o valor é próximo a 1, temos o indício de uma tendência positiva entre as variáveis, indicando que ao desenhar um scatterplot entre as duas variáveis, teremos os pontos próximos a uma linha reta crescente (coeficiente angular positivo). Enquanto que se o valor é próximo de -1, temos indícios de uma associação negativa. Por fim uma correlação próxima de zero significa que as observações não apresentam associações lineares, mas sim algo como uma nuvem de pontos aleatória.


```{r, warning=FALSE}
ggridges::lincoln_weather %>% 
  janitor::clean_names() %>%
  select(mean_temperature_f, mean_dew_point_f) %>% 
  plot()
```


```{r, warning=FALSE}
ggridges::lincoln_weather %>% 
  janitor::clean_names() %>%
  select(contains("mean_")) %>% 
  select(where(is.numeric)) %>% 
  GGally::ggpairs(progress = FALSE)
```


Um ponto importante, mas que não aprofundaremos aqui, é a relação entre o cálculo de medidas de dependência e os diferentes tipos de variáveis, considerando coeficientes de correlação de: Pearson, Spearman, Kendall, entre outros.

Por fim sempre importante lembrar que: correlação não significa causalidade! Ou seja, não é porque existe uma correlação forte entre dois atributos, que um irá necessariamente causar a mudança de movimento no outro. Dois termos interessantes para pesquisar e entender melhor sobre este tipo de fenômeno, são: variável de confusão e correlação espúria. Para esta última temos uma referência incrível para ilustrar este conceito: [Spurious Correlations](http://tylervigen.com/spurious-correlations).


# Considerações Finais sobre EDA

EDA é sobre interação! Quanto mais você investiga, mais informações terá para que os próximos passos da exploração sejam mais efetivos e **insightful** (esclarecedores). Aqui, ter uma abordagem curiosa e cientifica é extramemente recomendado. Basicamente, pensar em hipóteses, investiga-las, criar novas conjecturas, é uma excelente maneira de lidar com o dolorido fato de que: não sabemos o que vamos encontrar, e tá tudo bem! O John Tukey, um importante estatístico do século passado, visto por muitos como um dos pais da EDA, tem uma frase famosa que diz:  

>  “EDA is an attitude, a state of flexibility, a willingness to look for those things that we believe are not there, as well as those we believe to be there” (Tukey, 1986).

Em português seria algo como: "“EDA é uma atitude, um estado de flexibilidade, uma vontade de procurar por coisas que acreditamos que não existem, assim como por coisas que acreditamos que estão lá”, poético né? 

E para "procurar por coisas", quanto mais ferramentas você possuir, melhor. Aprender a arte de manusear, transformar e visualizar dados, por exemplo. E são estes os temas que veremos na sequência.

Até lá ;)





# HANDS-ON

- No R você irá encontrar dezenas de pacotes com o objetivo de "facilitar" a sua visão, particularmente para a exploração descritiva de dados. Bibliotecas como: `visdat`, `naniar`, `DataExplorer`, entre tantas outras. Que tal explorar um pouco estes pacotes?

- E em relação às distribuições de frequências, você já ouviu falar da distribuição Normal (Gaussiana)? Ou talvez da distribuição Poisson? Exponencial? Já refletiu sobre o porquê estudamos diferentes?
[Aplicativo Shiny para explorar distribuições de probabilidade](https://antoinesoetewey.shinyapps.io/statistics-101/)


